{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Perceptron.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ssd8EJ92yKu8",
        "colab_type": "text"
      },
      "source": [
        "# O que são Redes Neurais\n",
        "\n",
        "Muitas tarefas que envolvem inteligência, reconhecimento de padrões e detecção de objetos são extremamente difíceis de se automatizar, mes assim são facilmente desempenhadas por animais ou crianças.\n",
        "Por exemplo, como seu cachorro te reconhece, seu dono, ao invés de um estranho?\n",
        "Como uma criança consegue discernir entre um ônibus escolar a um ônibus municipal?\n",
        "E como nosso cérebro subconscientemente, reconhece padrões complexos a cada dia sem que se quer nos demos conta?\n",
        "\n",
        "\n",
        "**A resposta se encontra em nosso corpo**. Todos nós contemos, biologicamente, uma rede neural conectada a um sistema nervoso.\n",
        "\n",
        "O termo \"*Neural*\" é a forma adjetiva de \"*Neurônio*\", \"*Network*\" denomina uma estrutura do tipo grafo, logo o termo \"Artificial Neural Networks\" referencia um sistema computacional que visa replicar (ou no mínimo inspirado por) as conexões neurais em nosso sistema nervoso.\n",
        "\n",
        "Para um sistema ser considerado uma ANN(Artificial Neural Network) ele deve ter uma estrutura de grafos dirigidos onde cada nodo do grafo perfoma alguma computação simples. Cada conexão então carrega um sinal (a saída da computação anterior) de um nodo para outro, influenciados por um \"*weight*\" indicando se este sinal deve ser ampliado ou diminuído. Algumas conexões possuem pesos de valores altos e positivos indicando que este sinal é muito importante quando fazemos uma **classificação** , o inverso indicando que o sinal é pouco representativo para a **classificação** \n",
        "\n",
        "# Relação com a Biologia\n",
        "\n",
        "<p align=\"center\"><img src=\"https://github.com/dsantiago/Perceptron/raw/master/imgs/human-neuron.png\" height=\"400px\"></p>\n",
        "\n",
        "Nosso cérebro é composto por 10 bilhões de neurônios aproximadamente, cada um conectado a outros 10 bilhões. \n",
        "\n",
        "O corpo do neurônio(soma), onde as entradas(dendritos) e as saídas(axônio) conectam um soma a outro soma. \n",
        "Cada neurônio recebe um impulso eletroquímico de outros neurônios em seu dendrito. Se estes impulsos tiverem energia suficiente para ativar um neurônio, então este neurônio transmite o sinal através de seu axônio, enviando como entrada aos dendritos de outros neurônios, e estes por sua vez podem também ativar enviando a mensagem a diante.\n",
        "\n",
        "A grande sacada aqui é que esta ativação é uma **operação binária - o neurônio ativa ou não ativa**. Não existem níveis de ativação, simplificando: um neurônio só irá ativar caso sinal total recebido ultrapasse um certo limite.\n",
        "Sempre lembrando que o algoritmo de Redes Neurais, assim como Algoritmos Genéticos, são apenas inspirados em como funciona nosso organimos,  o primeiro levando em conta o cérebro e o segundo o material genético e possííveis mutações. Mas a verdade é que ainda não temos o domínio completo da neuro ciência, ou em palavras mais amistosas, a ciência do cérebro e por motivos óbvios nãão conseguimos replicar o funcionamento do cérebro de forma fidedigna.\n",
        "\n",
        "\n",
        "# Modelos Artificiais\n",
        "\n",
        "Uma rede neural simplesmente faz a soma ponderada das entradas. Os valores valores $X_1, X_2, X_3$ são nossas **entradas** e tipicamente corresponde a uma as linha de um *dataset* disposto de forma tabular.\n",
        "\n",
        "<p align=\"center\"><img src=\"https://github.com/dsantiago/Perceptron/raw/master/imgs/simple-nn.png\" height=\"400px\"></p>\n",
        "\n",
        "Cada $x$ é conectado a um neurônio com um vetor de peso **$W$** que consiste de $W_1, W_2, ..., W_n$, ou seja, cada entrada $x$ estáá associada a um peso $w$.\n",
        "\n",
        "Note que na última entrada passamos o valor $1$ associado a um $W_4$ este sendo uma entrada extra no vetor de pesos para atender este valor($1$). Esta entrada extra é chamada de *bias* e tem como motivação o conceito de **coeficiente linear** na equação da reta que é justo onde passa a reta corta o eixo $y$ dando maleabilidade de deslocamento vertical a reta.\n",
        "\n",
        "Por fim, a saída do dado, a direita da imagem, pega a soma ponderada pelos pesos, aplica uma função de ativação(usamos para identificar se o neurônio ativou ou não) e entrega o resultado.\n",
        "\n",
        "Matematicamente falando:\n",
        "\n",
        "* $ f\\left(X_1W_1 + X_2W_2 + X_nW_n\\right) $\n",
        "* $ f\\left(\\sum_{i=1}^{n} X_iW_i\\right) $\n",
        "* Ou simplesmente, $f(net)$, onde: $$ net = \\sum_{i=1}^{n} X_iW_i $$\n",
        "\n",
        "# Funções de Ativação\n",
        "\n",
        "A mais simples das funções de ativação é a \"*step function*\", utilizada pelo algoritmo no Perceptron.\n",
        "\n",
        "<br />\n",
        "\n",
        "$$\n",
        "f(net) = \n",
        "\\begin{cases}\n",
        "1  & \\text{if } net > 0\\\\\n",
        "0  & \\text{otherwise}\\\\\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "# O Algoritmo Perceptron\n",
        "\n",
        "Concebido por Rosenblatt em 1958, *The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain*<font color=\"#F00\">[1]</font> é provavelmente o mais antigo e simples dos algoritmos de ANN. Este paper sozinho éé imensamente responsável pela popularidade e utilidade das redes neurais nos dias atuais.\n",
        "\n",
        "Em 1969, um balde de água fria caiu na comunidade  de *Machine Learning* que quase pôs um fim ao avançço dos estudos na área. *Minsky e Papert* publicaram *Perceptrons: an introduction to computational geometry*<font color=\"#F00\">[2]</font>, um livro que estagnou os estudo em NN em quase uma década, mesmo sendo um livro<font color=\"#F00\">[5]</font> controverso, os autores demonstraram com sucesso que um Perceptron *single layer* éé ineficaz em separar dados não lineares.\n",
        "\n",
        "Mas logo na sequência, em 1970 onde começou-se a explorar redes mais profundas(chamadas também de *multi-layer perceptron*) em conjunto com o algoritmo de *backpropagation*(Webos<font color=\"#F00\">[3]</font> e Rumelhart<font color=\"#F00\">[4]</font> e logo o estudo em ANN saiu do hiato e voltou a aquecer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FmmmwLI5gWj",
        "colab_type": "text"
      },
      "source": [
        "# Problema do XOR\n",
        "\n",
        "Em um dataset *bitwise* contendo as operações *OR*, *AND* e *XOR* :\n",
        "\n",
        "| $x_0$ |  $x_1$ | | OR | AND | XOR |\n",
        "|--|--|--|--|--|--|\n",
        "| 0| 0|  | 0| 0| 0|\n",
        "| 0| 1|  | 1| 0| 1|\n",
        "| 1| 0|  | 1| 0| 1|\n",
        "| 1| 1|  | 1| 1| 0|\n",
        "\n",
        "Podemos notar que no caso do *OR* e *AND* podemos via uma linha classificar os dados porém no caso do *XOR* não existe linha que segmente os dados da forma correta, o *XOR* se caracteriza por não ser ***linearmente separável***.\n",
        "\n",
        "<p align=\"center\"><img src=\"https://github.com/dsantiago/Perceptron/raw/master/imgs/XOR_problem.png\" width=\"800\"></p>\n",
        "\n",
        "Não entendeu essa questão da lineariedade?\n",
        "\n",
        "Visualmente, veja na figura abaixo (extraída do MIT), sabemos de ante-mão que nosso algoritmo não conseguirá fazer a separação da segunda imagem.\n",
        "\n",
        "<p align=\"center\"><img src=\"https://github.com/dsantiago/Perceptron/raw/master/imgs/linear-vs-nao-linear.png\" width=\"800\"></p>\n",
        "\n",
        "# Premissas do Perceptron\n",
        "\n",
        "Nosso objetivo é identificar os pesos $w$ que classifique com precisão cada item em nossos dados. \n",
        "Para treinar nosso Perceptron devemos alimentar nossa rede com os dados *diversas vezes*. A cada vez que nossa rede enxergar todo o conjunto de dados, dissemos uma *epoch* se passou. Normalmente temos uma certa quantia de *epochs* até que nosso vetor de pesos $w$ possa ter sido aprendido/otimizado para separar linearmente nossos dados.\n",
        "\n",
        "Um pseudo-código pode ser visto a seguir:\n",
        "\n",
        "1. Inicializar o vetor $w$ com pequenos valores aleatórios <br />  \n",
        "2. Até o Perceptron convergir: <br />\n",
        "  (a) Itere o vetor $X$ e os valores verdadeiros $y$ <br />\n",
        "  (b) Para cada $X_i$, passe-o pela rede, calculando o valor de saída pela ***step function***: $\\hat{y}_i = \\phi\\left(W \\odot X_i\\right)$ <br />\n",
        "  (c) Atualize os pesos de $W$: $ W = W + \\left(-\\alpha\\left(\\hat{y}_i - y_i\\right)X_i\\right) \n",
        "  \\text{para todo } i \\in \\left\\{0,\\dotsc,n\\right\\} $ \n",
        "  \n",
        "O \"aprendizado\" ocorre nos passos 2b e 2c onde tomamos o produto interno de $w$ e obtemos  a saída $y_j$. Este valor então é passado a *step function* que nos retorna $1$ se $y > 0$ e $0$ caso contrário.\n",
        "\n",
        "Agora devemos mover nosso vetor $w$ um passo \"mais próximo\" da classificação correta, esta atualização é feita pelo que chamamos de *\"delta rule\"* no passo 2c.\n",
        "\n",
        "A expressão $\\left(\\hat{y} - y\\right)$ determina se nossa classificação está correta ou não, em caso possitivo a diferença deve ser $0$. Caso contrário esta diferença pode ser tanto positiva como negativa, nos dando a direção em que nossos pesos serão atualizados(e assim nos deixando mais próximos da classificação correta).\n",
        "\n",
        "Para atualizar nosso vetor $W$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6EhWsTWQp8K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np # Lib p/ Algebra Linear"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-guY6BsJkoEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Perceptron:\n",
        "    def __init__(self, N, alpha=0.1):\n",
        "        self.W = np.random.randn(N + 1) # inicializando com pesos aleatorios, um valor extra para o bias\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def step_function(self, x):\n",
        "        return 1 if x > 0 else 0\n",
        "\n",
        "    def fit(self, X, y, epochs=10):\n",
        "        X = np.c_[X, np.ones((X.shape[0]))]\n",
        "\n",
        "        for epoch in np.arange(0, epochs):\n",
        "            # loop sob cada entrada\n",
        "            for (x, target) in zip(X, y):\n",
        "                y_hat = self.step_function(np.dot(x, self.W)) # predicao\n",
        "\n",
        "                if y_hat != target:\n",
        "                    error = y_hat - target\n",
        "                    self.W += -self.alpha * error * x # atualiza o vetor de pesos\n",
        "    \n",
        "    def predict(self, X):\n",
        "        X = np.atleast_2d(X)\n",
        "        X = np.c_[X, np.ones((X.shape[0]))]\n",
        "\n",
        "        return self.step_function(np.dot(X, self.W))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EInIcTGbNZv1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "f6671b12-eae4-4f85-84ac-90c3ba668a61"
      },
      "source": [
        "print(\"[INFO] Valores binarios de entrada, representando 2 bits...\\n\")\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "X"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] Valores binarios de entrada, representando 2 bits...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0],\n",
              "       [0, 1],\n",
              "       [1, 0],\n",
              "       [1, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwA5rJ4uNSEU",
        "colab_type": "text"
      },
      "source": [
        "### Executando OR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwxxlKBFCFMJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "11520650-936c-41dd-bfe2-6a1d0979f81f"
      },
      "source": [
        "y = np.array([[0], [1], [1], [1]]) # Valores respostas do OR\n",
        " \n",
        "print(\"[INFO] Treinando Perceptron - OR...\")\n",
        "p = Perceptron(X.shape[1], alpha=0.1)\n",
        "p.fit(X, y, epochs=20)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] Treinando Perceptron - OR...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1e70iCpCIJO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "0b611389-f324-4b95-c5f6-e6963d8a02aa"
      },
      "source": [
        "print(\"[INFO] Predição Perceptron - OR... \\n\")\n",
        " \n",
        "for (x, target) in zip(X, y):\n",
        "\tpred = p.predict(x)\n",
        "\tprint(f\"[INFO] entrada={x}, valor-real={target[0]}, predicao={pred}\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] Predição Perceptron - OR... \n",
            "\n",
            "[INFO] entrada=[0 0], valor-real=0, predicao=0\n",
            "[INFO] entrada=[0 1], valor-real=1, predicao=1\n",
            "[INFO] entrada=[1 0], valor-real=1, predicao=1\n",
            "[INFO] entrada=[1 1], valor-real=1, predicao=1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeSNWjL3NrBT",
        "colab_type": "text"
      },
      "source": [
        "### Executando AND"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7WNxJL-WQR4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f9def425-b164-4325-8892-5101737f64a6"
      },
      "source": [
        "y = np.array([[0], [0], [0], [1]]) # Valores respostas do AND\n",
        " \n",
        "print(\"[INFO] Treinando Perceptron - AND...\")\n",
        "p = Perceptron(X.shape[1], alpha=0.1)\n",
        "p.fit(X, y, epochs=20)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] Treinando Perceptron - AND...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aURhGz0WaIm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "0dd24be9-8f37-47c2-c7a9-930bdedab860"
      },
      "source": [
        "print(\"[INFO] Predição Perceptron - AND... \\n\")\n",
        " \n",
        "for (x, target) in zip(X, y):\n",
        "\tpred = p.predict(x)\n",
        "\tprint(f\"[INFO] entrada={x}, valor-real={target[0]}, predicao={pred}\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] Predição Perceptron - AND... \n",
            "\n",
            "[INFO] entrada=[0 0], valor-real=0, predicao=0\n",
            "[INFO] entrada=[0 1], valor-real=0, predicao=0\n",
            "[INFO] entrada=[1 0], valor-real=0, predicao=0\n",
            "[INFO] entrada=[1 1], valor-real=1, predicao=1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pav7Cp21Wcjf",
        "colab_type": "text"
      },
      "source": [
        "### Executando XOR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmWKePpFWnit",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3090464d-a747-4cea-bc1c-e090808f13f4"
      },
      "source": [
        "y = np.array([[0], [1], [1], [0]]) # Valores respostas do XOR\n",
        " \n",
        "print(\"[INFO] Treinando Perceptron - XOR...\")\n",
        "p = Perceptron(X.shape[1], alpha=0.1)\n",
        "p.fit(X, y, epochs=20)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] Treinando Perceptron - XOR...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mu4rVS8qWrj2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "6595f304-6782-4640-b0c9-a4e1be6e540a"
      },
      "source": [
        "print(\"[INFO] Predição Perceptron - XOR... \\n\")\n",
        " \n",
        "for (x, target) in zip(X, y):\n",
        "\tpred = p.predict(x)\n",
        "\tprint(f\"[INFO] entrada={x}, valor-real={target[0]}, predicao={pred}\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] Predição Perceptron - XOR... \n",
            "\n",
            "[INFO] entrada=[0 0], valor-real=0, predicao=1\n",
            "[INFO] entrada=[0 1], valor-real=1, predicao=0\n",
            "[INFO] entrada=[1 0], valor-real=1, predicao=0\n",
            "[INFO] entrada=[1 1], valor-real=0, predicao=0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kh9RAjz_WtEk",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Percebe-se que o algoritmo não conseguiu prever este resultado justamente pela necessidade de termos um algoritmo não linear no caso do XOR, como já discutimos previamente. \n",
        "\n",
        "## E agora como resolver?\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYv98CSYXdE3",
        "colab_type": "text"
      },
      "source": [
        "# Multi-Layer ou Perceptron Multi Camadas\n",
        "\n",
        "A intuição inicial é a de que precisaremos adicionar *layers* ou camadas extras afim de resolver problemas mais complexos.\n",
        "\n",
        "Um exemplo de arquitetura básica de um Multi-layer Perceptron é a que segue:\n",
        "\n",
        "<p align=\"center\"><img src=\"https://github.com/dsantiago/Perceptron/raw/master/imgs/forward_pass.png\" height=\"400px\"></p>\n",
        "\n",
        "* No primeiro exemplo temos de 2 entradas, 2 *layers* ocultos e uma saída\n",
        "* No segundo exemplo temos de 3 entradas, 3 *layers* ocultos e novamente uma única saída\n",
        "\n",
        "## Função de Ativação\n",
        "\n",
        "Conferimos a *step function* anteriormente porém esta função não é diferenciável pois sóó pode assumir 0 ou 1 de forma abrupta, como podemos conferir na imagem a seguir:\n",
        "\n",
        "<p align=\"center\"><img src=\"https://github.com/dsantiago/Perceptron/raw/master/imgs/activation_functions.png\" height=\"600px\" ></p>\n",
        "\n",
        "Com excessão da *step function* (que mais parece um degrau de escada), todas as demais funções são diferenciáveis.\n",
        "\n",
        "Pare nosso caso, respeitando os papers iniciais, utilizaremos a função ***Sigmoid*** onde qualquer valor de entrada é achatado entre 0 e 1 porém de uma forma contínua. A fórmula da Simoid é dada por:\n",
        "\n",
        "<br />\n",
        "\n",
        "$$\n",
        "\\large\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "\\large\n",
        "$$\n",
        "\n",
        "O uso das funções de ativação nos ajuda a criar o efeito de não linearidade quando diversas são postas em cascata. Baseado na Teoria da Aproximação Universal: https://en.wikipedia.org/wiki/Universal_approximation_theorem.\n",
        "\n",
        "## O Backpropagation\n",
        "\n",
        "Backpropagation é muito provavelmente o algoritmo mais importante na história das Redes Neurais - sem (um eficiente) backpropagation, seria impossível treinar Deep Learning até a profundidade que vemos nos dias atuais. \n",
        "\n",
        "Backpropagation pode ser considerado o pilar das redes neurais/deep learning moderno.\n",
        "\n",
        "A incarnação original do backpropagation foi vista lá pelos idos de 1970, porém não foi até o seminal paper de 1986, \"*Learning representations by back-propagating errors*\" por Rumelhart, Hinton, e Williams, que foi possível inventar um algoritmo muito mais rápido emais adequado para treinar redes neurais profundas.\n",
        "\n",
        "O backpropagation consiste:\n",
        "* O \"forward pass\" onde nossas entradas são passadas pela rede e nossas predições obtidas.\n",
        "* O \"backward pass\" é onde where we computamos o gradiente da função de custo no último *layer*(camada)da rede a usa este gradiente para recursivamente aplicar a ***regra da cadeia*** para atualizar os pesos da nossa rede, também conhecida como fase de atualização dos pesos.\n",
        "\n",
        "\n",
        "No forward pass fazemos exatamente o que fizemos no Perceptron com o único adendo de ter mais camadas e, consequentemente, torna-se um sistema linear mais complexo.\n",
        "\n",
        "No backward pass, temos que ter uma função diferenciável, ou seja, derivável. Só assim conseguiremos calcular as derivadas parciais envolvidas do erro em respeito a um peso $W_{i,j}$, $loss(E)$, saída $O_i$ e a saída da rede $net_i$.\n",
        "\n",
        "<br />\n",
        "\n",
        "$$ \n",
        "\\frac{\\partial E}{\\partial W_{i,j}} = \\frac{\\partial E}{\\partial O_i} \\frac{\\partial O_i}{\\partial net_i}  \n",
        "\\frac{\\partial net_i}{\\partial W_{i,j}}\n",
        "$$\n",
        "\n",
        "Como o objetivo não é explicar as derivadas envolvidas, porém para os aficcionados por matemática, segue  a resolução.\n",
        "\n",
        "Nos conformamos em entender que a derivada da Sigmoid tem como resultado:\n",
        "\n",
        "Derivada Simóide:\n",
        "\n",
        "$$\n",
        "\\large\n",
        "\\sigma'(x) = \\sigma(x)\\left(1 - \\sigma(x)\\right)\n",
        "\\large\n",
        "$$\n",
        "\n",
        "Prova:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\dfrac{d}{dx} \\sigma(x) &= \\dfrac{d}{dx} \\left[ \\dfrac{1}{1 + e^{-x}} \\right] \\\\\n",
        "&= \\dfrac{d}{dx} \\left( 1 + \\mathrm{e}^{-x} \\right)^{-1} \\\\\n",
        "&= -(1 + e^{-x})^{-2}(-e^{-x}) \\\\\n",
        "&= \\dfrac{e^{-x}}{\\left(1 + e^{-x}\\right)^2} \\\\\n",
        "&= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\dfrac{e^{-x}}{1 + e^{-x}}  \\\\\n",
        "&= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\dfrac{(1 + e^{-x}) - 1}{1 + e^{-x}}  \\\\\n",
        "&= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\left( \\dfrac{1 + e^{-x}}{1 + e^{-x}} - \\dfrac{1}{1 + e^{-x}} \\right) \\\\\n",
        "&= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\left( 1 - \\dfrac{1}{1 + e^{-x}} \\right) \\\\\n",
        "&= \\sigma(x) \\cdot (1 - \\sigma(x))\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "## Atualizando os pesos\n",
        "\n",
        "Uma vez percorrido reversamente a rede da última cada até a primeira e calculando os gradientes baseados nas derivadas parciais de cada camada, vamos atualizar nosso vetor de pesos $W$. Para isso usamos um learning rate $\\alpha$ pequeno para dara pequeno passos em direção do ponto ótimo da função de custo, no nosso caso simplesmente $error=\\left(\\hat{y} - y\\right)$.\n",
        "\n",
        "Podemos concluir que caminhamos o vetor de pesos com seus gradientes escalados pelo fator $\\alpha$.\n",
        "\n",
        "<br />\n",
        "\n",
        "$$\n",
        "\\large\n",
        "w_{t + 1} = w_t  - \\alpha.\\nabla w_t\n",
        "\\large\n",
        "$$\n",
        "\n",
        "#Implementação"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdHXrymqtysj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AU9nasYlivw1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiLayerPerceptron:\n",
        "    def __init__(self, layers, alpha=0.1):\n",
        "        self.W = []\n",
        "        self.layers = layers # Camadas em forma de lista. Ex: [2,2,1]\n",
        "        self.alpha = alpha # Taxa de aprendizado\n",
        "\n",
        "        for i in np.arange(0, len(layers) - 2):\n",
        "            # No ex. [2,2,1] o resultado das 2 primeiras camdas seria uma matrix 2x2, \n",
        "            # porém adicionando o bias temos uma 3x3\n",
        "            w = np.random.randn(layers[i] + 1, layers[i + 1] + 1)\n",
        "            self.W.append(w)\n",
        "\n",
        "        # A úúltima camada não precisamos adicionar o bias\n",
        "        w = np.random.randn(layers[-2] + 1, layers[-1])\n",
        "        self.W.append(w)\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1.0 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_deriv(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def calculate_loss(self, X, targets):\n",
        "        targets = np.atleast_2d(targets)\n",
        "        predictions = self.predict(X, addBias=False)\n",
        "        loss = 0.5 * np.sum((predictions - targets) ** 2)\n",
        "        return loss\n",
        "\n",
        "    def fit(self, X, y, epochs=1000, displayUpdate=100):\n",
        "        # Adicionando o bias (coluna de 1's)\n",
        "        X = np.c_[X, np.ones((X.shape[0]))]\n",
        "\n",
        "        for epoch in np.arange(0, epochs):\n",
        "            \n",
        "            for (x, target) in zip(X, y):\n",
        "                self.fit_partial(x, target)\n",
        "\n",
        "            if epoch == 0 or (epoch + 1) % displayUpdate == 0:\n",
        "                loss = self.calculate_loss(X, y)\n",
        "                print(f\"[INFO] epoch={epoch + 1}, loss={loss:.7f}\")\n",
        "\n",
        "    def fit_partial(self, x, y):\n",
        "        X = [np.atleast_2d(x)]\n",
        "\n",
        "        # Feedfoward:\n",
        "        for layer in np.arange(0, len(self.W)):\n",
        "            net = np.dot(X[layer], self.W[layer])\n",
        "            out = self.sigmoid(net) # Nossa ativação não linear\n",
        "            X.append(out)\n",
        "\n",
        "        # Backpropagation\n",
        "        \n",
        "        # Calculamos o erro da última camada e seguimos recursivamente da última até a primeira\n",
        "        error = X[-1] - y \n",
        "        # Calculamos a seguir o nossa lista de gradiente, tendo como 1 elemnto o erro * gradiente da última camada\n",
        "        grad = [error * self.sigmoid_deriv(X[-1])] \n",
        "\n",
        "        # Executamos as derivadas parciais a partir da última camada (aplicação da regra da cadeia)\n",
        "        for layer in np.arange(len(X) - 2, 0, -1):\n",
        "            delta = np.dot(grad[-1], self.W[layer].T)\n",
        "            delta = delta * self.sigmoid_deriv(X[layer])\n",
        "            grad.append(delta)\n",
        "\n",
        "        # Reverte os dados, visto que foram gerados de trás p/ frente\n",
        "        grad = grad[::-1]\n",
        "\n",
        "        # Atualiza os pesos com os gradientes com passos determinados pela Learning Rate alpha\n",
        "        for layer in np.arange(0, len(self.W)):\n",
        "            self.W[layer] += -self.alpha * X[layer].T.dot(grad[layer])\n",
        "\n",
        "    def predict(self, X, addBias=True):\n",
        "        y_hat = np.atleast_2d(X)\n",
        "        \n",
        "        if addBias:\n",
        "            y_hat= np.c_[y_hat, np.ones((y_hat.shape[0]))]\n",
        "\n",
        "        for layer in np.arange(0, len(self.W)):\n",
        "            y_hat = self.sigmoid(np.dot(y_hat, self.W[layer]))\n",
        "\n",
        "        return y_hat"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYkTnudeiwMb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "62c89ef8-ed0e-4390-d706-f43d392934f9"
      },
      "source": [
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "print(\"Entradas XOR...\\n\", X)\n",
        "print(\"Saídas XOR...\\n\", y)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Entradas XOR...\n",
            " [[0 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [1 1]]\n",
            "Saídas XOR...\n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vonNrmAsiweM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "7f5a021d-27cb-45fe-eeca-d6230ce92138"
      },
      "source": [
        "nn = MultiLayerPerceptron([2, 2, 1], alpha=0.5)\n",
        "nn.fit(X, y, epochs=20000, displayUpdate=1000)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] epoch=1, loss=0.7513548\n",
            "[INFO] epoch=1000, loss=0.1814224\n",
            "[INFO] epoch=2000, loss=0.1332085\n",
            "[INFO] epoch=3000, loss=0.1290036\n",
            "[INFO] epoch=4000, loss=0.1275399\n",
            "[INFO] epoch=5000, loss=0.1267793\n",
            "[INFO] epoch=6000, loss=0.1262695\n",
            "[INFO] epoch=7000, loss=0.1257879\n",
            "[INFO] epoch=8000, loss=0.1245960\n",
            "[INFO] epoch=9000, loss=0.0085606\n",
            "[INFO] epoch=10000, loss=0.0023704\n",
            "[INFO] epoch=11000, loss=0.0014009\n",
            "[INFO] epoch=12000, loss=0.0009999\n",
            "[INFO] epoch=13000, loss=0.0007790\n",
            "[INFO] epoch=14000, loss=0.0006385\n",
            "[INFO] epoch=15000, loss=0.0005410\n",
            "[INFO] epoch=16000, loss=0.0004694\n",
            "[INFO] epoch=17000, loss=0.0004144\n",
            "[INFO] epoch=18000, loss=0.0003709\n",
            "[INFO] epoch=19000, loss=0.0003356\n",
            "[INFO] epoch=20000, loss=0.0003064\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0rYqJl7u-v5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "8433a1bb-7e94-4170-8761-f0ea44805c3e"
      },
      "source": [
        "for (x, target) in zip(X, y):\n",
        "\tpred = nn.predict(x)[0][0]\n",
        "\tstep = 1 if pred > 0.5 else 0\n",
        "\tprint(f\"[INFO] data={x}, valor-real={target[0]}, predicao={pred:.4f}, step_function={step}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] data=[0 0], valor-real=0, predicao=0.0139, step_function=0\n",
            "[INFO] data=[0 1], valor-real=1, predicao=0.9888, step_function=1\n",
            "[INFO] data=[1 0], valor-real=1, predicao=0.9869, step_function=1\n",
            "[INFO] data=[1 1], valor-real=0, predicao=0.0110, step_function=0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1nM8OHKvrnE",
        "colab_type": "text"
      },
      "source": [
        "## Finalmente conseguimos criar uma rede neural que se adapta a problemas não lineares e o XOR foi vencido."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3Ll89ltiwyj",
        "colab_type": "text"
      },
      "source": [
        "# REFERÊNCIAS\n",
        "\n",
        "<font color=\"#F00\">[1]</font> F. Rosenblatt. “The Perceptron: A Probabilistic Model for Information Storage and Organization in The Brain”. In: Psychological Review (1958), pages 65–386.\n",
        "\n",
        "<font color=\"#F00\">[2]</font> M. Minsky and S. Papert. Perceptrons. Cambridge, MA: MIT Press, 1969.\n",
        "\n",
        "<font color=\"#F00\">[3]</font> P. J.Werbos. “Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences”. PhD thesis. Harvard University, 1974.\n",
        "\n",
        "<font color=\"#F00\">[4]</font> David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. “Neurocomputing: Foundations of Research”. In: edited by James A. Anderson and Edward Rosenfeld. Cambridge, MA, USA: MIT Press, 1988. Chapter Learning Representations by Back-propagating Errors, pages 696–699. ISBN: 0-262-01097-6. URL: http://dl.acm.org/citation.cfm?id=65669.104451.\n",
        "\n",
        "<font color=\"#F00\">[5]</font> Mikel Olazaran. “A Sociological Study of the Official History of the Perceptrons Controversy”.In: Social Studies of Science 26.3 (1996), pages 611–659. ISSN: 03063127. URL: http://www.jstor.org/stable/285702.\n",
        "\n",
        "<font color=\"#F00\">[6]</font> Rumelhart, Hinton, and Williams. \"Learning Representations by back-propagation errors\" http://www.cs.toronto.edu/~hinton/absps/naturebp.pdf"
      ]
    }
  ]
}